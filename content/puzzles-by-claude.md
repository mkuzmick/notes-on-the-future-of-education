# Puzzles and Questions

Open questions for rethinking higher education in the age of AI.

---

## The Core Shift

The shifts are interconnected. In curriculum, we move from outputs to systems. In learning, from production to judgment. In work, from doing to deciding. In the university's role, from knowledge holder to epistemic trainer. And underneath it all, scarcity migrates: first we lacked texts, then access to texts, then the ability to select among texts—now, increasingly, the scarcity is *caring about texts at all*.

These questions try to map that terrain through three interconnected problematics: the migration of scarcity, the problem of authorization, and the question of formation.

---

## I. The Scarcity Migration

We are drowning in the competent. AI produces fluent prose, serviceable analysis, plausible arguments—infinite amounts, on demand. The scarcity that once defined knowledge work has migrated elsewhere. Understanding where it's gone is the first task.

### What's the value of excellence when competence is free?

If competence is now a commodity, where does value migrate? Presumably toward genuine excellence, true innovation, work that breaks frames rather than filling them.

This parallels Kuhn's distinction between normal science and revolutionary science: AI excels at puzzle-solving within established paradigms, but paradigm shifts require something else—the recognition that the puzzle itself is wrong, the willingness to abandon productive frameworks, the insight that comes from dissatisfaction rather than facility.

If that's right, what does it mean for education? Are we still training students for "normal" knowledge work—competent performance within established frames—when that work is increasingly automated? How do we cultivate the capacity for genuine novelty, for recognizing when the paradigm needs breaking? And is that even teachable, or does it emerge only from deep immersion in a tradition followed by productive rebellion against it?

### What happens when text becomes a utility?

Text is becoming like water or electricity—abundant, on-demand, undifferentiated. What happens to expertise when its primary outputs are commodified? How do the economics of information abundance change what it means to be "expert"?

### What is the university for, now that it's no longer where the books are?

For centuries, the university held a near-monopoly: on texts, on the training of knowledge workers, on the certification of expertise. To learn was to go where the knowledge was, and the knowledge was in libraries, laboratories, and the minds of credentialed faculty. The university didn't just transmit knowledge—it *was* knowledge, in institutional form.

AI breaks this arrangement. Knowledge is now abundant, fluent, and available on demand. The training of knowledge workers is no longer captive to any institution. Expertise can be simulated, if not replicated. The scarcity has migrated elsewhere.

So what remains? If the university is no longer the gatekeeper of information, the sole trainer of professionals, or the exclusive site of intellectual community—what is it for? Is it a finishing school for elites? A certification authority? A place for formation that can't happen elsewhere? A research engine that still requires concentration of resources? All of these? None?

### Why should anyone accept the university's authority?

If the university's value shifts from possessing knowledge to deciding what counts, a harder question follows: why should anyone accept that *this* institution's decisions matter? Canon-making is rhetorically strong but structurally fragile. The university isn't just competing with abundant content; it's competing with abundant *critique* of its own authority.

What grounds the university's claim to decide what matters? Is it process? Expertise? Tradition? And are any of these stable in a world where AI can generate sophisticated counter-arguments on demand?

### Should the university become an epistemic notary?

There's an alternative to canon-making: certification of process rather than judgment of value. The university as notary, auditor, witness—guaranteeing not "this is good" but "this was produced under conditions we can verify."

This is less authoritarian and more defensible. It's compatible with AI-assisted critique of authority because it doesn't claim to be the arbiter of truth, only the guarantor of epistemic conditions. The degree becomes a kind of epistemic notarization: this person worked through a process we can account for.

But does this go far enough? Is process certification satisfying, or does it evacuate the university's traditional claim to actually know something about quality? Can you have institutional authority without substantive judgment?

---

## II. The Authorization Problem

A degree used to certify that you could *do* the work. Increasingly, it will certify that you can *authorize* the work—take responsibility for outputs you didn't produce from scratch. This shift has implications for credentialing, pedagogy, professional formation, and ethics.

### How do you authorize work you didn't do?

Can you really train someone to authorize work they've never done themselves? What does it mean to sign your name to something a machine wrote?

The failure mode shifts too: from "I didn't finish" to "I didn't notice." The old failure was about production—you didn't complete the work. The new failure is about attention—the flaw was there, and you missed it. How do we train students for a world where the cost of inattention is higher than the cost of incapacity?

If the human in the loop isn't just "editing" but is legally and ethically responsible for the output, how do we prepare students for the *weight* of that responsibility—not just the workflow of it?

### What does a degree mean now?

A degree used to certify that you could do the work: write the brief, run the analysis, diagnose the patient. It was a license to practice, backed by demonstrated competence. But if AI can do much of that work, what exactly does the degree certify?

Several possibilities, none mutually exclusive: That you can *authorize* work, not just produce it. That you've been *exposed* to a tradition of inquiry. That you've been *socialized* into professional norms and networks. That you've developed *judgment* about when to trust outputs and when to question them. That you've acquired *taste*—the ability to distinguish the competent from the excellent. That you've been *formed* as a person in ways that matter for citizenship and life, not just employment.

Some of these the university can defend. Others it has always claimed but rarely assessed. And some may be better achieved elsewhere—through apprenticeship, online credentials, or professional communities. If the degree is no longer a proxy for "can do the work," what is it a proxy for? And can the university articulate that value proposition clearly enough to justify the time and cost?

### How do experts develop without doing junior work?

You can't train supervisors who never did the work being supervised. You can't develop taste without making bad art first. You can't cultivate judgment without making and learning from mistakes. So: what replaces "learning by doing bad versions first" as a mechanism for developing expertise?

### What happens to intellectual ownership?

Call it "epistemic agency"—the ability to own a thought you didn't technically originate. When AI drafts, synthesizes, and proposes, and the human selects, refines, and approves, who is the author? What does originality mean when the raw material is machine-generated?

This isn't just a legal or ethical question about plagiarism. It's a question about the formation of intellectual identity. How do students develop a sense of *their own* ideas when the line between prompted output and personal insight blurs? Can you build intellectual confidence on a foundation of curation rather than creation? And what happens to the felt experience of *having* an idea—the moment of insight—when that moment is increasingly collaborative with machines?

### How do we teach care and attention?

Authorization has both intellectual and moral dimensions. The intellectual side is about competence: can you verify the work is sound? The moral side is about care: did anyone actually *attend* to this?

In the past, receiving a human-authored document meant someone somewhere had given it attention—drafted, revised, considered, approved. That implicit guarantee is gone. Now the person who finally *cares* about a piece of work may be the first human to do so. Being that person is an ethical act, not just a quality-control function.

How do we teach students to be the ones who care? The skills required—sustained attention, genuine engagement, the willingness to slow down and actually *look*—are the same ones the university has always tried to cultivate. But they matter more now, because you can no longer assume anyone else has done it. How do we prepare students for the moral weight of being the first human in the loop?

### Is ethics now inseparable from competence?

If AI can produce competent work that is subtly flawed—hallucinated citations, plausible but unsupported causal claims, confident prose covering uncertain ground—then spotting those flaws becomes essential to quality control. Does this mean ethics and epistemology are no longer separate from disciplinary training?

Should we stop teaching ethics as a standalone course and start treating it as inseparable from competence? In this world, if a student's work is unethical or logically flawed, it isn't a separate problem—it's simply *broken*.

---

## III. The Formation Question

Not everything should change. Some things the university does become more valuable precisely because they resist automation. The question is which slownesses, which frictions, which traditions are worth protecting—and why.

### How do we transmit intellectual desire?

AI can simulate interest. It can't transmit desire. Universities teach people to care—but how? What actually happens when a student falls in love with the archive, or can't let go of a question years after the course ends?

If genuine intellectual desire is the scarcest resource in an age of infinite competent output, can it be taught? Or only caught—through encounter with people who already care? And if it's the latter, what institutional structures protect the space for that encounter?

### What is the university's role in identity formation?

College has long been understood as a place where you find yourself, build yourself, become who you are. This isn't just about acquiring skills or credentials—it's about the formation of a person. The residential college, the late-night conversation, the encounter with difference, the freedom to experiment and fail.

How does this change when AI can be a constant interlocutor, advisor, and mirror? Does the availability of infinite patient feedback change the nature of self-discovery? What happens to identity formation when you can always ask a machine "who should I become?"—and get a plausible answer?

The question isn't whether AI replaces human mentorship. It's whether the *texture* of becoming a person changes when certain kinds of friction disappear. And if identity formation remains central to the university's mission, how do we protect and cultivate it in this new environment?

### Which slownesses are worth protecting?

The university offers a different relationship to time: the requirement that some things take time because the *taking of time* is part of what produces the result. But which slownesses are essential, and which are merely traditional?

Not everything that takes time deserves to. How do we distinguish friction (what we want to eliminate) from temporal structure (what we want to protect)? What forms of learning genuinely require duration, and what forms are just inefficient?

### When should students think without AI?

There is a certain kind of thinking that only happens when you are struggling with a blank page. There is a certain kind of discovery that only occurs in the friction of doing it yourself. If this is true, how do we teach students when *not* to use AI?

The skill isn't just "when to delegate to AI" but "when to protect a space from AI entirely." How do you develop the metacognitive awareness to know the difference? And how do institutions create spaces where unassisted struggle is valued rather than inefficient?

### What embodied, material, or relational skills become more important?

As symbolic work gets automated, what becomes more valuable? The things AI can't do: physical presence, material craft, relational attunement. What does this mean for curriculum? For how we think about "practical" versus "intellectual" education?

### What is the role of live dialogue and face-to-face encounter?

In an age of asynchronous AI tutoring, what can only happen in real time, between humans, in a room? What practices from the humanities, arts, and sciences are worth preserving precisely because they don't scale?

---

## IV. Methods and Mechanisms

Given the three problematics above—the migration of scarcity, the authorization problem, the formation question—what concrete changes to teaching and assessment follow?

### Should we bring back the oral defense?

If the paper is no longer proof of thinking, the only way to establish intellectual responsibility may be to put the human in the hot seat. Does this mean a return of the oral defense—not to prove you did the work (the work is cheap now), but to prove you can *account* for the work?

What would it look like to train students to defend decisions they didn't technically originate—to sit before a panel and walk through the logical tests they ran, the alternatives they discarded, the risks they accepted? Is this the new form of academic rigor?

### Should the validation log become a standard assignment?

If students are working with AI, perhaps every assignment should include a validation log: What did the AI produce? What did you verify? What did you change, and why? What risks remain?

This shifts assessment from product to process. The log becomes evidence of judgment, attention, and responsibility—proof that a human actually engaged with the output rather than just accepting it. If the AI hallucinates a citation and the student doesn't catch it, they haven't just made an error; they've demonstrated they aren't yet qualified to authorize.

But does this create perverse incentives? Does it bureaucratize the creative process? And what about work where the goal is seamless integration—where you *don't* want to see the seams?

### What counts as verifiable now?

As AI-generated text floods the zone, disciplines will increasingly be defined by their "ground truth" protocols—the things AI can't hallucinate: raw field data, physical lab results, primary archival discovery. But what does this mean for fields where the primary material is already textual?

If the verifiable audit trail becomes as important as the final output, how does assessment change? Do we shift from "is this essay good?" to "can you show me how it was made?"—and what do we lose if we do?

### Should students learn to design inquiry, not just execute it?

Faculty expertise is increasingly about staging inquiry—designing conceptual frames, curating corpora and methods, evaluating *why* a system produces a result. If this is what experts actually do, should undergraduates encounter research as system design much earlier?

What would it look like to teach students to *construct* arguments rather than write them, to *shape* models rather than run them? And does this require abandoning the traditional paper as the privileged endpoint of learning?

### How do we teach taste?

In an age of infinite average outputs, the student's job is to move the needle from competent to exceptional. This requires taste—knowing what good looks like, having preferences you can defend, recognizing quality before you can articulate why. But is taste teachable?

How do you develop a point of view rather than just a critical eye? Is it exposure to examples? Guided discussion? Practice making judgments that others evaluate? And how do we assess something as personal as taste without flattening it into rubrics?

### Can we now expect dramatically more from students?

If AI handles the scaffolding, students can attempt things that were previously impossible at undergraduate scale. Instead of reading one novel and excerpts of others, a student could build an interpretive system that analyzes a thousand pulp novels from the 1890s—and then defend the system's design and findings. Instead of writing a paper about a concept, a student could create an interactive website that lets viewers explore the idea themselves.

This isn't about making things easier. It's about raising the ceiling. While we should preserve traditional practices where they matter—the in-class essay, the oral presentation, the unassisted problem set—we can also expect every student to be capable of things that previously required teams or professional tools. The final project isn't "write about X" but "build something that makes X legible to others."

What does this mean for curriculum? For prerequisites? For what counts as "advanced" work? If a first-year student can, with AI assistance, produce artifacts that previously required senior-level skills, how do we sequence learning? And how do we ensure they're actually learning, not just prompting?

### How can we leverage AI for feedback at scale?

Faculty must remain responsible for grades—that's part of the authorization structure. But between "no feedback" and "final grade," there's an enormous space that AI can fill. Students could have access to sophisticated feedback systems at every step of the process—not just a tutor-bot that answers questions, but genuine diagnostic tools.

Consider how much we know about baseball swings or golf strokes—sports with the budgets to build robust tech-enabled feedback systems that track mechanics, identify weaknesses, and suggest corrections in real time. We can now build equivalents for writing, for understanding French literature, for intro stats problem sets, for quantum physics. Systems that don't just say "this is wrong" but show *where* the reasoning broke down, *how* the argument could be stronger, *what* patterns suggest the student is struggling with a particular concept.

The question is how to design these systems well. What metrics matter for learning, as opposed to performance? How do we avoid teaching to the AI's preferences rather than genuine understanding? And how do faculty stay in the loop—designing the feedback systems, interpreting the patterns, making the judgments that matter—without being overwhelmed by data they can't process?

### Is scholarship becoming something you experience, not just read?

If AI can generate competent prose on demand, the paper stops being the privileged endpoint of learning. Expect more performative scholarship: demos, walkthroughs, annotated systems. More emphasis on process artifacts—prompts, datasets, decision logs. Increased legitimacy of multimodal academic communication.

Scholarship becomes something that can be *experienced*, not just read. What does this mean for how we train students? For how we assess them? For what counts as "academic work"?

---

## Additional Questions

These don't fit neatly into the three problematics but may be useful for discussion:

- Should "forensic reading"—the ability to find where AI breaks—become a core skill across disciplines?
- What role does risk assessment play in the new curriculum? Are we training cognitive risk managers?
- How do we teach prompt engineering and AI steering without reducing education to tool use?
- Can adversarial review (red-teaming AI outputs) become a standard pedagogical method?
- How do we compress the apprenticeship phase without eliminating it entirely?
- What does "demonstration of mastery" look like when production is cheap but judgment is expensive?
- Should assessment shift from "what do you know?" to "how do you know it?" and "why should I trust you?"
- Can sustained engagement with difficulty be taught, or only required?
- How do we protect the "luxury" of slowness without making it a privilege only some students can afford?
- What can universities do that other institutions (corporations, online platforms, professional associations) cannot?
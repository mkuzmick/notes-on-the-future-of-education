# Puzzles and Questions

Open questions for rethinking higher education in the age of AI.

The shifts are interconnected. In curriculum, we move from outputs to systems. In learning, from production to judgment. In work, from doing to deciding. In the university's role, from knowledge holder to epistemic trainer. And underneath it all, a pattern we might call *scarcity migration*: first we lacked texts, then access to texts, then the ability to select among texts—now, increasingly, the scarcity is *caring about texts at all*. Each generation solves the previous generation's bottleneck, only to discover that the constraint has moved upstream.

These questions try to map that terrain.

## Big Changes to Knowledge Work and the University's Role

Routine cognitive work—summarizing, drafting, analyzing, translating, coding—is becoming radically cheap. Sam Altman borrows the old nuclear-era promise: "intelligence too cheap to meter." Whether or not that prediction holds exactly, the direction is clear. And it puts pressure on every knowledge profession and every institution that trains people for them. Before we can rethink education, we need to understand what's actually changing—both in the work itself and in the university's relationship to it.

#### What happens when work shifts from production to judgment?

Consider what a senior faculty member actually does. They don't typically write first drafts from scratch — they design studies, evaluate arguments, decide what questions are worth asking, and judge whether the results hold up. They produce less and judge more. AI is accelerating this pattern across every knowledge profession: the routine production work — drafting, summarizing, analyzing, translating — gets cheaper, while the judgment work stays expensive.

If students are headed for careers that look more like what faculty already do — less production, more evaluation — what does this mean for the classroom? The traditional model assumes students build judgment by doing large volumes of production work first. But if AI handles much of that production, where does judgment come from? How can we train students in judgment if they don't go through the laborious, repeated manual production that earlier generations required to become expert?

#### From authoring to authorizing — and what a degree certifies

The words share a root, but the work is diverging. To *author* is to originate—to produce the text, the analysis, the argument from scratch. To *authorize* is to take responsibility for it: to review, to certify, to sign your name. A degree used to certify that you could do the first. Increasingly, it may certify something closer to the second—that you can take responsibility for outputs you didn't produce from scratch, that you've developed *judgment* about when to trust them and when to question them, that you've acquired *taste*, that you've been *formed* as a person in ways that matter for citizenship and life, not just employment.

But can you really train someone to authorize work they've never authored themselves? And the failure mode shifts: from "I didn't finish" to "I didn't notice." The old failure was about production—you didn't complete the work. The new failure is about attention—the flaw was there, and you missed it. If the human in the loop isn't just "editing" but is legally and ethically responsible for the output, how do we prepare students for the *weight* of that responsibility—not just the workflow of it?

Some of what a degree might now certify, the university can defend. Others it has always claimed but rarely assessed. And some may be better achieved elsewhere—through apprenticeship, online credentials, or professional communities. If the degree is no longer a proxy for "can do the work," what is it a proxy for? And can the university articulate that value clearly enough to justify the time and cost?

#### What's the value of excellence when competence is free?

We are drowning in the competent. AI produces fluent prose, serviceable analysis, plausible arguments—infinite amounts, on demand. If competence is now a commodity, where does value migrate? Presumably toward genuine excellence, true innovation, work that breaks frames rather than filling them.

What does this mean for education? Are we still training students for competent performance within established frames—when that work is increasingly automated? How do we cultivate the capacity for genuine novelty? And is excellence even teachable, or does it emerge only from deep immersion in a tradition followed by productive rebellion against it?

#### What is the university for now?

For centuries, the university held a near-monopoly on texts, on the training of knowledge workers, on the certification of expertise. It didn't just transmit knowledge—it *was* knowledge, in institutional form. AI breaks this arrangement. Knowledge is now abundant, fluent, and available on demand. Expertise can be simulated, if not replicated. The scarcity has migrated elsewhere.

So what remains? If the university is no longer the gatekeeper of information, the sole trainer of professionals, or the exclusive site of intellectual community—what is it for? One possibility: the university as epistemic notary—certifying not "this is good" but "this was produced under conditions we can verify." The degree as epistemic notarization: this person worked through a process we can account for. But does process certification go far enough, or does it evacuate the university's traditional claim to actually know something about quality? Can you have institutional authority without substantive judgment?

#### What happens to intellectual ownership?

Call it "epistemic agency"—the ability to own a thought you didn't technically originate. When AI drafts, synthesizes, and proposes, and the human selects, refines, and approves, who is the author? What does originality mean when the raw material is machine-generated?

This isn't just a legal or ethical question about plagiarism. It's a question about the formation of intellectual identity. How do students develop a sense of *their own* ideas when the line between prompted output and personal insight blurs? Can you build intellectual confidence on a foundation of curation rather than creation? And what happens to the felt experience of *having* an idea—the moment of insight—when that moment is increasingly collaborative with machines?

#### Additional questions

- What happens when text becomes a utility—like water or electricity—rather than a scarce product of skilled labor?
- How do the economics of information abundance change what it means to be "expert"?
- If AI produces the average of all human thought, does expertise shift from knowing the most to being the most divergent from the mean?
- What can universities do that other institutions (corporations, online platforms, professional associations) cannot?

## Changes in What Students Need to Learn

Given the shifts above, what skills and dispositions should education actually develop? The traditional disciplinary apprenticeship may no longer fit the work students are preparing for.

#### How do experts develop without doing junior work?

You can't train supervisors who never did the work being supervised. You can't develop taste without making bad art first. You can't cultivate judgment without making and learning from mistakes. So: what replaces "learning by doing bad versions first" as a mechanism for developing expertise?

There's a deeper catch-22 here. AI is most useful to people who already know enough to evaluate its outputs—to spot the hallucination, to recognize when the analysis is superficial, to know what good looks like. But the process of acquiring that knowledge is exactly what AI short-circuits. The tool assumes a foundation it prevents you from building. How do we get students to the point where AI is genuinely useful to them, if the path there requires the kind of unassisted struggle that AI makes easy to skip?

#### Should students learn to design systems of inquiry, interpretation, and analysis?

Faculty expertise is increasingly about designing the whole apparatus — choosing the conceptual frame, curating the corpus, selecting the methods, evaluating *why* a system produces a result rather than just whether it does. If this is what experts actually do, should undergraduates encounter research as system design much earlier?

What would it look like to teach students to *construct* arguments rather than write them, to *shape* models rather than run them, to *architect* an interpretive system rather than execute a single analysis? And does this require abandoning the traditional paper as the privileged endpoint of learning?

#### Should we be teaching taste?

The humanities once made taste central — the cultivation of judgment about what is excellent, what endures, what repays attention. Over recent decades, the field moved away from that project, and for understandable reasons: questions of canon and quality were entangled with questions of power and exclusion. But AI may force the issue back open. We are now drowning in slop — fluent, plausible, machine-generated content that is competent enough to pass and empty enough to waste everyone's time. If no one can distinguish the excellent from the merely adequate, the slop wins by volume.

Taste — knowing what good looks like, having preferences you can defend, recognizing quality before you can articulate why — may be more crucial now than at any point since the university stopped explicitly teaching it. How do you develop a point of view rather than just a critical eye? Is it exposure to examples? Guided discussion? Practice making judgments that others evaluate? And how do we teach taste without retreating to the old gatekeeping, while still insisting that not everything AI produces deserves our attention?

#### How do we teach care and attention?

Authorization has both intellectual and moral dimensions. The intellectual side is about competence: can you verify the work is sound? The moral side is about care: did anyone actually *attend* to this?

In the past, receiving a human-authored document meant someone somewhere had given it attention—drafted, revised, considered, approved. That implicit guarantee is gone. Now the person who finally *cares* about a piece of work may be the first human to do so. Being that person is an ethical act, not just a quality-control function.

How do we teach students to be the ones who care? The skills required—sustained attention, genuine engagement, the willingness to slow down and actually *look*—are the same ones the university has always tried to cultivate. But they matter more now, because you can no longer assume anyone else has done it. How do we prepare students for the moral weight of being the first human in the loop?

#### Do students know when they actually know something?

AI makes it easy to perform understanding you don't have. A student can produce fluent work on a topic they barely grasp, pass a problem set with assistance they couldn't replicate alone, or articulate a position they couldn't defend under questioning. The result is a new kind of epistemic risk: *false fluency*—the illusion of mastery where only surface competence exists.

This isn't just a cheating problem. Even well-intentioned students may not be able to tell whether they've internalized something or merely outsourced it. When AI handles the scaffolding, the felt experience of "getting it" can be indistinguishable from the felt experience of having a capable tool. The struggle that used to signal the gap between not-knowing and knowing—the frustration, the false starts, the slow click of comprehension—gets smoothed away.

If that's right, students need a new metacognitive skill: the ability to honestly assess whether they *know* something or merely *have access* to something. This is harder than it sounds, because AI collapses the distance between the two. And it suggests that intrinsic motivation—genuine curiosity, the desire to understand rather than to perform understanding—becomes more important than ever. Extrinsic incentives (grades, credentials, completion) can all be satisfied without learning. Only the student who actually *wants* to know has a reliable reason to find out whether they do.

How do we cultivate that self-awareness? How do we help students distinguish between "I can produce this" and "I understand this"? And how do we build a culture where admitting you don't yet know something is valued over performing as if you do? Underneath the metacognitive question is a motivational one: do students *care* whether they actually know? If the system rewards output regardless of understanding, why would anyone pause to check? The false fluency problem isn't just cognitive—it's a crisis of caring.

#### Additional questions

- Should "forensic reading"—the ability to find where AI breaks—become a core skill across disciplines?
- What role does risk assessment play in the new curriculum? Are we training cognitive risk managers?
- How do we teach prompt engineering and AI steering without reducing education to tool use?
- What embodied, material, or relational skills become *more* important as symbolic work gets automated?
- How do we teach intellectual stamina—the capacity to sit with a problem for five days when a serviceable answer is five seconds away?

## What the University Should Preserve

Not everything should change. Some things the university does become more valuable precisely because they resist automation. The question is which slownesses, which frictions, which traditions are worth protecting—and why.

#### How do we transmit intellectual desire?

AI can simulate interest. It can't transmit desire. Universities teach people to care—but how? What actually happens when a student falls in love with the archive, or can't let go of a question years after the course ends?

If genuine intellectual desire is the scarcest resource in an age of infinite competent output, can it be taught? Or only caught—through encounter with people who already care? And if it's the latter, what institutional structures protect the space for that encounter?

#### Can college teach students to break paradigms, not just learn them?

AI is exceptionally good at *normal* intellectual work — in Kuhn's terms, puzzle-solving within established paradigms. What it doesn't do, or doesn't yet do, is recognize when the paradigm itself needs breaking — the kind of insight that comes from dissatisfaction rather than facility, from noticing that the puzzle is wrong rather than solving it faster. College may be one of the few environments structured to cultivate that capacity: deep immersion in a discipline, cross-disciplinary encounter, protection from market pressure, and time to pursue questions without immediate payoff.

But most undergraduates are still learning the paradigm, not breaking it. Can the educational experience plant the seeds of productive rebellion — the willingness to question frameworks that are working, the dissatisfaction that drives genuine novelty — even before students have fully mastered the tradition? Or does paradigm-breaking only emerge later, and college's job is to build the foundation deep enough that it can eventually be broken?

#### What is the university's role in identity formation?

College has long been understood as a place where you find yourself, build yourself, become who you are. This isn't just about acquiring skills or credentials—it's about the formation of a person. The residential college, the late-night conversation, the encounter with difference, the freedom to experiment and fail.

How does this change when AI can be a constant interlocutor, advisor, and mirror? Does the availability of infinite patient feedback change the nature of self-discovery? What happens to identity formation when you can always ask a machine "who should I become?"—and get a plausible answer?

The question isn't whether AI replaces human mentorship. It's whether the *texture* of becoming a person changes when certain kinds of friction disappear. And if identity formation remains central to the university's mission, how do we protect and cultivate it in this new environment?

#### Which slownesses are worth protecting?

The university offers a different relationship to time: the requirement that some things take time because the *taking of time* is part of what produces the result. But which slownesses are essential, and which are merely traditional?

Not everything that takes time deserves to. How do we distinguish friction (what we want to eliminate) from temporal structure (what we want to protect)? What forms of learning genuinely require duration, and what forms are just inefficient?

#### When should students think without AI?

There is a certain kind of thinking that only happens when you are struggling with a blank page. There is a certain kind of discovery that only occurs in the friction of doing it yourself. If this is true, how do we teach students when *not* to use AI?

The skill isn't just "when to delegate to AI" but "when to protect a space from AI entirely." How do you develop the metacognitive awareness to know the difference? And how do institutions create spaces where unassisted struggle is valued rather than inefficient?

#### Additional questions

- What is the role of live dialogue, oral tradition, and face-to-face encounter in an age of asynchronous AI tutoring?
- Can sustained engagement with difficulty be taught, or only required?
- What practices from the humanities, arts, and sciences are worth preserving precisely because they don't scale?
- How do we protect the "luxury" of slowness without making it a privilege only some students can afford?
- Is boredom a prerequisite for original thought? If AI eliminates intellectual boredom, do we lose a primary driver of curiosity?

## Changes in Methods of Teaching and Assessment

Given what we want students to learn (III) and what we want to preserve (IV), how do the actual methods of teaching and assessment need to change?

#### What counts as verifiable now?

As AI-generated text floods the zone, disciplines will increasingly be defined by their "ground truth" protocols—the things AI can't hallucinate: raw field data, physical lab results, primary archival discovery. But what does this mean for fields where the primary material is already textual?

If the verifiable audit trail becomes as important as the final output, how does assessment change? Do we shift from "is this essay good?" to "can you show me how it was made?"—and what do we lose if we do?

#### Should we bring back the oral defense?

If the paper is no longer proof of thinking, the only way to establish intellectual responsibility may be to put the human in the hot seat. Does this mean a return of the oral defense—not to prove you did the work (the work is cheap now), but to prove you can *account* for the work?

What would it look like to train students to defend decisions they didn't technically originate—to sit before a panel and walk through the logical tests they ran, the alternatives they discarded, the risks they accepted? Is this the new form of academic rigor?

#### Should the validation log become a standard assignment?

If students are working with AI, perhaps every assignment should include a validation log: What did the AI produce? What did you verify? What did you change, and why? What risks remain?

This shifts assessment from product to process. The log becomes evidence of judgment, attention, and responsibility—proof that a human actually engaged with the output rather than just accepting it. If the AI hallucinates a citation and the student doesn't catch it, they haven't just made an error; they've demonstrated they aren't yet qualified to authorize.

But does this create perverse incentives? Does it bureaucratize the creative process? And what about work where the goal is seamless integration—where you *don't* want to see the seams?

There's a deeper problem: even well-designed process assignments are vulnerable to the tools they're meant to govern. Students can use AI to generate plausible validation logs, reflective narratives, and documentation of "struggle" they never experienced. The log becomes compliance theatre—a performance of diligence rather than evidence of it. This suggests that no purely textual assignment can fully verify that learning happened. At some point, the student has to be *present*: defending choices in real time, answering unexpected questions, demonstrating understanding that can't be scripted in advance. The oral defense, the live demonstration, the in-class exercise—these aren't nostalgic holdovers. They may be the only reliable ways to close the gap between what a student can submit and what a student actually knows.

#### What does learning look like when you're working with AI?

Students increasingly learn *with* entities that respond, surprise, drift, hallucinate, and improve with feedback. This is a new kind of pedagogical relationship—not teacher and student, not student and textbook, but something stranger.

This pushes learning toward dialogic models: conversation, interrogation, steering. It requires metacognition—"why did it do that?"—and explicit modeling of assumptions and constraints. The assignment becomes less "write a paper about X" and more "design a system that reliably produces insight about X, then explain where it fails."

Does this mean learning becomes closer to studio or lab models, even in text-heavy fields? More exploratory, less linear? And if so, what does that mean for curriculum design?

#### Is scholarship becoming something you experience, not just read?

If AI can generate competent prose on demand, the paper stops being the privileged endpoint of learning. Expect more performative scholarship: demos, walkthroughs, annotated systems. More emphasis on process artifacts—prompts, datasets, decision logs. Increased legitimacy of multimodal academic communication.

Scholarship becomes something that can be *experienced*, not just read. What does this mean for how we train students? For how we assess them? For what counts as "academic work"?

#### Is ethics now inseparable from competence?

If AI can produce competent work that is subtly flawed—hallucinated citations, plausible but unsupported causal claims, confident prose covering uncertain ground—then spotting those flaws becomes essential to quality control. Does this mean ethics and epistemology are no longer separate from disciplinary training?

Should we stop teaching ethics as a standalone course and start treating it as inseparable from competence? In this world, if a student's work is unethical or logically flawed, it isn't a separate problem—it's simply *broken*.

#### How can we leverage AI for feedback at scale?

Faculty must remain responsible for grades—that's part of the authorization structure. But between "no feedback" and "final grade," there's an enormous space that AI can fill. Students could have access to sophisticated feedback systems at every step of the process, not just a tutor-bot that answers questions, but genuine diagnostic tools.

Consider how much we know about baseball swings or golf strokes—sports with the budgets to build robust tech-enabled feedback systems that track mechanics, identify weaknesses, and suggest corrections in real time. We can now build equivalents for writing, for understanding French literature, for intro stats problem sets, for quantum physics. Systems that don't just say "this is wrong" but show *where* the reasoning broke down, *how* the argument could be stronger, *what* patterns suggest the student is struggling with a particular concept.

The question is how to design these systems well. What metrics matter for learning, as opposed to performance? How do we avoid teaching to the AI's preferences rather than genuine understanding? And how do faculty stay in the loop—designing the feedback systems, interpreting the patterns, making the judgments that matter—without being overwhelmed by data they can't process?

#### Can we now expect dramatically more from students?

If AI handles the scaffolding, students can attempt things that were previously impossible at undergraduate scale. Instead of reading one novel and excerpts of others, a student could build an interpretive system that analyzes a thousand pulp novels from the 1890s—and then defend the system's design and findings. Instead of writing a paper about a concept, a student could prompt an interactive website that lets viewers explore the idea themselves.

This isn't about making things easier. It's about raising the ceiling. While we should preserve traditional practices where they matter—the in-class essay, the oral presentation, the unassisted problem set—we can also expect every student to be capable of things that previously required teams or professional tools. The final project isn't "write about X" but "build something that makes X legible to others."

What does this mean for curriculum? For prerequisites? For what counts as "advanced" work? If a first-year student can, with AI assistance, produce artifacts that previously required senior-level skills, how do we sequence learning? And how do we ensure they're actually learning, not just prompting?

#### Additional questions

- Can adversarial review (red-teaming AI outputs) become a standard pedagogical method?
- How do we compress the apprenticeship phase without eliminating it entirely?
- What does "demonstration of mastery" look like when production is cheap but judgment is expensive?
- Should assessment shift from "what do you know?" to "how do you know it?" and "why should I trust you?"

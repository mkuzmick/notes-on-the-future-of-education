# Oral Exams in the Age of AI

A faculty briefing on assessment, pedagogy, and the recovery of an ancient tradition.

## Executive Summary

Written assessment in higher education faces an epistemic crisis. With 88% of UK undergraduates now using generative AI in their coursework[^1] — up from 53% just one year earlier — and detection tools achieving only 33–81% accuracy, the written essay can no longer reliably indicate whether a student understands the material they have submitted. As one NYU student told *The New Yorker* in June 2025, "Any type of writing in life, I use A.I."[^2] When asked if this constituted cheating, he replied: "Of course. Are you fucking kidding me?"

This briefing argues that oral assessment is not merely a defensive response to generative AI but a recovery of higher education's founding pedagogy, supported by robust evidence from cognitive science, decades of scholarship of teaching and learning (SoTL), and the continuous practice of elite professional schools. For five hundred years — from the founding of the University of Bologna in 1088 through the eighteenth century — oral disputation was not a supplement to academic assessment; it was the assessment.[^3] Written examinations emerged in the 1700s and 1800s driven by the scale demands of industrialization, not by any pedagogical discovery that writing tests understanding better than speech.

Harvard is uniquely positioned to lead this recovery. The University already practices oral assessment at its highest levels: the Business School has graded oral discussion for 118 years,[^4] the Medical School requires Objective Structured Clinical Examinations for every MD student,[^5] and concentrations like History & Literature,[^6] Social Studies,[^7] and Folklore & Mythology[^8] have required senior oral exams for decades. The question is not whether oral assessment works at Harvard — it demonstrably does — but whether the University will extend what works in pockets to broader undergraduate education.

This document synthesizes 65+ sources across eight research domains: the history of oral examinations, peer institutional responses to AI, Harvard's own oral traditions, SoTL evidence on speaking and assessment, post-AI classroom experiments worldwide, the role of AI tools in oral evaluation, philosophical foundations from Plato to Derrida, and professional domain practices in law, medicine, and business. The evidence converges on a single conclusion: **oral assessment cultivates genuine understanding and professional readiness, and AI-resistance is a bonus, not the primary rationale.**

## The Historical Case: Five Hundred Years of Oral Assessment

#### The Medieval University Was an Oral Institution

The modern university was born oral. When the University of Bologna was founded in 1088 and the University of Paris emerged around 1150, the entire curriculum was organized around three oral practices: *Lectio* (reading aloud and commenting on authoritative texts), *Disputatio* (structured oral debate on philosophical and theological questions), and *Quaestio* (resolving contradictions between authorities through oral argument). For the next five centuries, oral disputation was not supplementary to the assessment of learning — it was the assessment itself. Cambridge's official history records that for over 500 years, examinations were "public, oral, and in Latin."[^9]

This was not a failure of imagination. Medieval educators understood something contemporary cognitive science has rediscovered: that dialogue reveals understanding in ways monologue cannot. When a student must defend a thesis against a skilled interlocutor — responding to objections, clarifying ambiguities, synthesizing across domains in real time — they demonstrate a quality of understanding that a written document, however polished, cannot fully capture. The medieval *disputatio* tested precisely what AI now makes it impossible to verify through writing alone: whether the person who produced the argument actually understands it.

#### The Shift to Written Exams: A Story of Scale, Not Pedagogy

Written examinations did not displace oral assessment because anyone demonstrated they were pedagogically superior. As Christopher Stray has documented, Cambridge's mathematics tripos pioneered written tests in the eighteenth century because the growing number of students made individual oral examinations logistically burdensome.[^10] Other disciplines adopted written exams not through independent evaluation but through institutional imitation. The Industrial Revolution's demand for standardized credentialing accelerated this shift: factories needed certified workers, and written tests could process hundreds of candidates efficiently. Oral exams could not.

The key insight is that American higher education's abandonment of undergraduate oral assessment was geographically and historically specific — not a universal marker of progress. The PhD *viva voce* was never abandoned anywhere in the world. The French *Classes Préparatoires* still conduct twice-weekly oral examinations across all subjects, meaning elite French students sit for 200+ oral exams before university entrance.[^11] India's civil service examination — the gateway to the nation's most prestigious administrative positions — culminates in a mandatory 275-mark viva voce.[^12] German universities maintained oral STEM examinations throughout the twentieth century. Italian universities conducted five oral exams per day during COVID. The rest of the world kept practicing what we stopped.

#### What Drove the American Abandonment

In the United States, the Progressive Education movement of the early twentieth century delivered the final blow to routine oral assessment. John Dewey's emphasis on experiential learning, while philosophically rich, was implemented in ways that devalued formal oral performance and memorization. A 1953 *Atlantic* essay by Albert Lynd captures the cultural moment: traditional education's emphasis on oral recitation was swept aside in favor of "growth and richness and joy." The irony is that Dewey's own pragmatism — his insistence that "knowledge is begotten and exercised in action" — actually supports oral assessment, which tests knowledge-in-action rather than knowledge-as-artifact. But Progressive Education's implementation narrowed the pedagogical repertoire, and oral examination was lost.

The lesson for 2026: oral assessment's decline in American undergraduate education was a contingent institutional choice, driven by ideology and logistics, not by evidence that writing tests understanding better than speech. Other advanced democracies — France, India, Japan, Germany, the United Kingdom — maintained or expanded oral examination. The conditions that drove the American shift (industrial scale, Progressive ideology) no longer obtain at a university with a 9:1 student-faculty ratio and a tutorial system designed for intensive dialogue.

## The Cognitive Science Case: Why Speaking Deepens Learning

The argument for oral assessment does not rest solely on AI-resistance. Three decades of scholarship of teaching and learning provide robust evidence that oral assessment is pedagogically superior for certain outcomes — synthesis, argumentation, real-time problem-solving — regardless of any technological disruption.

#### The Production Effect

The production effect, documented extensively in cognitive psychology, demonstrates that speaking information aloud significantly improves long-term memory encoding compared to silent reading or writing.[^13] When students articulate ideas verbally, they engage multimodal encoding: auditory processing of their own voice, motor engagement of speech production, and self-referential processing that tags the memory as distinctly "mine." This is not a minor enhancement. Studies consistently show that produced items (spoken aloud) are remembered 10–15% better than silently read items, a difference that compounds across a semester of oral practice.

For faculty considering oral assignments, the production effect means that the *preparation* for an oral exam may be more educationally valuable than the exam itself. Students who know they must articulate and defend arguments verbally engage differently with course material: they rehearse explanations, anticipate objections, and organize knowledge for real-time retrieval rather than for the linear, revisable format of an essay.

#### The Protégé Effect

The protégé effect — the finding that students who prepare to teach material learn it more deeply than those who prepare only to be tested — has particular relevance for oral assessment design.[^14] When students anticipate explaining concepts to an examiner (a quasi-teaching scenario), they process material at a deeper level: identifying core principles, building explanatory frameworks, and connecting ideas across domains. Critically, research shows that the least able students benefit most from this effect, suggesting oral assessment could narrow rather than widen achievement gaps when properly scaffolded.

#### Deep Learning and Cognitive Apprenticeship

Iannone and Simpson's influential study found that oral assessment shifts students from surface to deep learning approaches.[^15] The unpredictability of dialogue — not knowing exactly which question will come next or how the examiner will probe a response — prevents the rote preparation strategies that written exams incentivize. Students cannot memorize their way through an oral exam the way they can through a written one; they must understand material well enough to discuss it from any angle.

A 2024 study in *Frontiers in Education* frames this as "cognitive apprenticeship": oral exams create a setting where expert examiners make their own thinking visible through follow-up questions, and students learn to externalize the invisible cognitive processes of their discipline.[^16] This is precisely what a written exam cannot do — a static prompt elicits a static response, with no opportunity for the kind of iterative refinement that characterizes genuine intellectual exchange.

#### The Oral-Written Complementarity

A January 2025 meta-analysis in the *Review of Educational Research* examined the relationship between oral language competency and written composition, finding that the two skills are mutually reinforcing rather than competing.[^17] Students who develop oral fluency become better writers because both skills require clarity of thought, logical organization, and audience awareness. The relationship strengthened as students progressed through higher levels of education and was particularly pronounced among foreign language learners. This demolishes the false dichotomy that oral assessment "replaces writing instruction." The evidence shows it enhances writing by developing the underlying cognitive competencies both modes share.

## The Global Landscape: How Peer Institutions Are Responding

The world's leading universities have responded to generative AI with a spectrum of approaches, from prohibition to integration. The trajectory is remarkably consistent: initial bans give way to enforcement failure, which leads to assessment redesign. Oral assessment emerges repeatedly as the structural endpoint.

#### The Prohibition-to-Redesign Arc

Oxford and Cambridge banned ChatGPT in March 2023. By April 2025, a Cambridge MPhil instructor reported: "We moved to essentially oral exams… let students use AI as much as they want, given the condition that anything AI does wrong is considered their fault."[^18] This quiet retreat from prohibition to structural redesign is the defining pattern of 2023–2026. Yale produced a 121-page AI report and committed $150 million, but left classroom policy to individual faculty — resulting in student confusion and inconsistent expectations. Northwestern's student newspaper reported "confusion and opportunity" as each professor implemented different policies. McGill found that 88% of students used AI in Fall 2025, and responded by shifting all courses to in-person exams.[^19]

#### Institutions That Named Oral Assessment

Several peer institutions have gone further, explicitly naming oral examination as a legitimate AI-era assessment format in institutional policy:

**Stanford**'s AI Working Group explicitly recommended "in-person formats such as oral exams" for AI-limited assessment,[^20] establishing oral examination as the university's sensible default alongside a framework treating AI use as equivalent to collaboration requiring instructor permission.

**The National University of Singapore** (ranked #8 globally) wrote oral interviews directly into institutional AI policy: "If there is a need to test whether you possess a certain knowledge or capability without access to AI tools, your instructors will continue to arrange for appropriate assessment settings (e.g., an on-site proctored exam or oral interview)."[^21] NUS professors now say trying to catch AI use is a "lost cause" — but rather than despair, they redesigned assessment.

**The University of British Columbia**'s Faculty of Arts launched the "We're Only Human" program in February 2026, defining three approaches: AI-secure assessments (including "oral responses"), AI-open assessments, and hybrid models.[^22] The program title acknowledges that faculty and students are navigating uncertainty together — a more sophisticated framing than the adversarial "professors vs. cheaters" narrative.

**Cornell**'s English Language Support Office requires that students "be prepared to orally discuss any content included in your paper."[^23] This is elegant policy design: it does not mandate curriculum overhaul but shifts the burden to students. If you submit writing, you must be able to explain it orally. The mere expectation deters pure AI generation, builds oral fluency, and scales through spot-checking.

#### The South African Reframe

Perhaps the most important strategic insight comes from South African scholarship. A December 2023 study in *Cogent Education* argued: "The perceived threat of ChatGPT to traditional assessment should spur professors to innovate and improve their assessments."[^24] Drawing parallels to 1970s calculator debates, the authors noted that educators initially feared students would lose arithmetic skills — but eventually, curriculum adapted. The reframe is crucial: instead of "ChatGPT ruined assessment, what do we do?" the question becomes "ChatGPT exposed assessment weaknesses, how do we improve?" A California community college that shifted all finals in-person found that optional oral components were actually *more accessible* for students with test anxiety and learning disabilities — challenging the assumption that oral exams are inherently exclusionary.[^25]

## Harvard's Own Traditions: We Already Do This

Harvard does not need to invent oral assessment. It needs to scale what it already practices in pockets across the University. The institution's own history and current operations provide every necessary precedent.

#### The Founding Pedagogy

Colonial Harvard (1636–1770s) was built on "forensic disputations" — oral debates conducted as public performances. The curriculum centered on the lecture, the declamation, and the disputation, all oral forms. Harvard Library archives document oral assessment as the dominant mode through the 1800s. When the University was founded, there was no other way to verify that a student had genuinely mastered the liberal arts: you asked them, in person, and they answered.

#### Concentrations That Never Stopped

**History & Literature** (founded 1906) requires a one-hour senior oral exam: five prepared topics, three faculty examiners, and a tradition that has produced alumni including Conan O'Brien and Frank Rich.[^26] The concentration also requires three mandatory oral presentations in sophomore tutorials, graded on a six-category rubric covering summary, source analysis, cross-source connections, contextualization, discussion questions, and presentation style. This is the French *colles* model implemented at Harvard: low-stakes formative practice building toward a high-stakes capstone.

**Social Studies** requires a 1.5-hour comprehensive oral: thesis defense plus a general exam on the concentration's core course.[^27] Students write a 1,000-word "intellectual autobiography" in preparation. The exam has been mandatory for all seniors since 1960 — 66 years of continuous oral assessment demonstrating both feasibility and rigor.

**Folklore & Mythology** (founded 1967) requires all concentrators — not just honors students — to complete a general examination with both written and oral components in their final term.[^28] The concentration's focus on oral traditions, oral literature, and oral performance makes the assessment format philosophically aligned with the subject matter. If a concentration studying how human societies preserved knowledge through spoken word requires oral examination, the principle extends naturally to any discipline where understanding matters more than artifact production.

#### Professional Schools That Never Stopped

Harvard's professional schools present an intra-university paradox: they all require oral competency assessment while the College primarily assesses through writing.

**Harvard Business School** has used the case method since 1908, devoting up to 80% of class time to oral discussion and grading class contribution at 40–50% of course grades.[^29] With approximately 900 MBA students and case discussions in every required course, HBS conducts thousands of hours of oral assessment annually. If the Business School can grade oral contribution for all students across two years, the College can assess 6,700 undergraduates.

**Harvard Medical School** requires OSCEs — clinical simulation exercises using standardized patients, objective rubrics, and multiple testing stations — for every MD student.[^30] HMS addresses the "subjectivity" objection directly: OSCEs are classified as "quantitative" assessment with structured scoring. If oral-performative assessment can be rigorous enough for life-or-death medical competency, it is rigorous enough for undergraduate coursework.

**Harvard Kennedy School**'s Master in Public Policy program culminates in the Policy Analysis Exercise, with oral presentations each spring. **Harvard Law School** pioneered the Socratic method in 1870 — 156 years of oral pedagogy making live dialogue the foundation of legal education.

The strategic question writes itself: if three Harvard professional schools representing business, medicine, and public policy — fields with very different epistemologies — all require oral competency assessment, what justifies the College's near-exclusive reliance on written work? The remaining objections ("undergrads aren't ready," "it doesn't scale," "it's too subjective") are refuted by History & Literature requiring sophomore oral work, HBS assessing 900 students annually for 118 years, and HMS proving oral exams can be quantitative with structured rubrics.

## Philosophical Foundations: From Plato to Derrida

The case for oral assessment is not merely practical. A 2,400-year philosophical tradition illuminates why dialogue tests understanding in ways that writing cannot — and why each new communication technology triggers anxieties about authenticity that ultimately prove productive.

#### Plato's Phaedrus and the First "Phaedrus Moment"

In the *Phaedrus* (c. 370 BCE), Socrates tells the story of the Egyptian god Theuth, who invented writing and presented it to King Thamus as a gift for humanity. Thamus rejected the offer: writing, he argued, "will implant forgetfulness in their souls; they will cease to exercise memory because they rely on that which is written, calling things to remembrance no longer from within themselves, but by means of external marks." Writing produces "the appearance of wisdom, not the reality."[^31]

Socrates's critique of writing maps uncannily onto the current AI disruption. Replace "writing" with "generative AI" and the passage reads as a 2026 op-ed: students will cease to exercise understanding because they rely on that which is AI-generated, producing the appearance of knowledge, not the reality. The parallel suggests we are living through another "Phaedrus moment" — a technology-induced crisis of authentication that reveals what assessment was supposed to test all along.

Socrates's solution was the same one this briefing proposes: dialogue. The Socratic *elenchus* — cross-examination through iterative questioning — reveals what is genuinely understood by testing whether the speaker can defend claims under pressure, respond to objections, and follow implications to their conclusions. Written texts, Socrates insisted, "when asked, maintain a solemn silence." Only in live dialogue can understanding be authenticated.

#### Walter Ong and the Oral Mind

Walter Ong's *Orality and Literacy* (1982) demonstrated that oral cultures think differently from literate ones: their knowledge is aggregative, redundant, participatory, and close to the human lifeworld.[^32] Ong's work suggests that oral assessment doesn't just test the same knowledge in a different format — it tests *different cognitive capacities*. Written assessment privileges linear, abstract, decontextualized reasoning. Oral assessment privileges synthesis, responsiveness, embodied argumentation, and the ability to think *with* an interlocutor rather than *at* an audience.

#### Derrida, Street, and the Ideology of Writing

Jacques Derrida's critique of "phonocentrism" complicates any simple romanticization of speech over writing — and this complication is intellectually productive. Derrida argued that speech is not inherently superior to writing; both are representations, both are mediated, both are subject to misinterpretation. This is a useful corrective: oral assessment is not a return to some pristine state of authentic communication. It is a *different mode of assessment* with different strengths and different vulnerabilities.

Brian Street's New Literacy Studies adds a further dimension.[^33] Street distinguished between the "autonomous model" of literacy (treating writing as a neutral, context-independent skill) and the "ideological model" (recognizing that what counts as "good writing" is shaped by power structures, disciplinary conventions, and social context). If we accept Street's analysis, the very idea that written assessment is "more objective" than oral is itself an ideological assumption — a product of institutional inertia rather than empirical evidence. Written and oral assessment are equally context-dependent; the choice between them is political and pedagogical, not epistemological.

#### Cicero's Five Canons: What We Lost

Cicero's *De Oratore* (55 BCE) identified five canons of rhetoric: Invention (generating arguments), Arrangement (organizing ideas), Style (expressing clearly), Memory (internalizing knowledge for retrieval), and Delivery (embodying argument physically).[^34] Modern written assessment tests at most three of these: Invention, Arrangement, and Style. It has abandoned Memory (open-book exams and research papers allow external retrieval) and Delivery (no physical performance required). Oral assessment recovers the full Ciceronian model — and with it, the comprehensive intellectual formation that the liberal arts tradition was designed to provide.

## The Scalability Question: Evidence That It Works

The most common faculty objection to oral assessment is logistical: "It can't scale." International evidence and emerging technology demolish this claim.

#### Proof at Scale

**CQUniversity, Australia** conducted 600 oral exams in under one week for a first-year literacy course in 2023.[^35] Students created reflective journals and then verbally shared their learning. The logistics were managed through scheduling tools and structured rubrics. If an Australian regional university can orally examine 600 first-year students in five days, Harvard can manage its tutorial sections.

**Georgia Tech's "Socratic Mind"** platform has been piloted with over 5,000 students, using AI to conduct Socratic questioning with dynamic follow-ups.[^36] Between 70% and 95% of students reported positive experiences. The platform demonstrates that AI can serve as a practice partner and formative assessment tool, reserving human examiners for high-stakes summative evaluation.

**NYU's Voice AI Experiment** used an ElevenLabs voice agent to conduct 25-minute oral exams at a cost of $0.42 per student, with 89% grade alignment with human examiners.[^37] The grading system used a "council of LLMs" (Claude, Gemini, and ChatGPT deliberating together) to reduce individual model bias. While the equity and validity implications require further study, the cost barrier has been obliterated.

**The French Precedent** is perhaps the most powerful: Classes Préparatoires conduct twice-weekly oral exams across all subjects for two years — roughly 200+ oral sessions per student before university entrance.[^38] This proves oral assessment scales when it is embedded as normal pedagogy rather than treated as a high-stakes exception. Harvard's tutorial system already provides the infrastructure for this model: weekly sections of 8–12 students, led by trained teaching fellows, are ideally sized for regular oral check-ins.

#### AI as Ally, Not Replacement

Emerging tools suggest a division of labor between AI and human examiners. The **Sherpa** tool, developed by Stanford undergraduates, transcribes student oral responses, flags potentially weak answers, and routes them to instructors for review — enabling asynchronous scheduling and reducing faculty time burden.[^39] Commercial platforms like InStage and VirtualSpeech provide AI-powered practice environments where students can rehearse oral defenses and receive automated feedback on argumentation, clarity, and delivery before facing human examiners. The model that emerges is layered: AI for low-stakes practice and formative feedback, human judgment for high-stakes summative assessment.

## Equity, Anxiety, and Honest Challenges

A responsible case for oral assessment must engage candidly with its real limitations. Oral exams are not universally better — they are *different*. Some students will perform worse orally than in writing, and without careful design, oral assessment can introduce new forms of inequity.

#### Bias in Unstructured Oral Assessment

The evidence on bias is clear and sobering. Accent, gender, race, and articulateness all influence unstructured oral grading. CU Boulder's NSF-funded research found 24% lower automatic speech recognition accuracy for Black speakers, and Ohio State analysis showed AI grading systematically compresses score distributions — lenient on low-performing work, harsh on high-performing work.[^40] Additional research documents ASR bias against disfluent speech: pauses, self-corrections, and "um"s are misrepresented in transcription, and disfluency correlates with anxiety, ESL status, and neurodiversity.[^41]

However, a rigorous 2024 study in *Frontiers in Education* demonstrated that structured rubrics eliminate measurable grading bias. Pre-service teachers with negative implicit associations toward students with migration backgrounds graded them worse — but *only when no rubric was used*. When objective criteria were provided, no grading disparities were found.[^42] The solution is not to avoid oral assessment but to never implement it without explicit, predetermined criteria.

#### Student Anxiety

Student anxiety about oral exams is real and documented. A 2021 study identified six themes driving fear: judgment, physical symptoms, uncertainty, comparison with peers, need for practice, and desire for support. But the same research points to the solution: scaffolding throughout the semester. When students receive repeated low-stakes practice with constructive feedback, anxiety decreases markedly. The History & Literature model — three graded oral presentations in sophomore year building toward a senior oral exam — is pedagogically sound anxiety mitigation.

It is also worth noting the counter-evidence: a California community college that introduced optional oral exam components found that many students *preferred* them to written tests because timed writing under pressure was actually more stressful for students with test anxiety and learning disabilities.[^43] The assumption that oral exams are inherently more anxiety-inducing than written exams deserves scrutiny.

#### Accessibility and Accommodation

Oral assessment must be designed with accommodations from the start, not retrofitted. For deaf and hard-of-hearing students, AI-powered real-time captioning and human-reviewed transcripts can make oral exams accessible in ways that were not possible a decade ago. For students with speech impairments or severe social anxiety, written alternatives must remain available. For multilingual learners, a 2026 scoping review found that oral assessment can actually *advantage* them by separating content knowledge from English writing proficiency — speaking fluency develops faster than academic writing for second-language learners.

The design imperative is clear: rubrics that score content understanding, not accent or grammar polish; multiple examiners to reduce individual bias; recordings for review and appeals; extended time for those who need it; and the option for visual aids and translated prompts. Harvard has the resources and the institutional infrastructure to do this right.

## What the Professions Already Know

Every elite profession that cares about genuine competency — where performance matters, not just credentials — has maintained or developed sophisticated oral assessment. Their practices offer Harvard both precedent and practical models.

#### Medicine: OSCEs as the Gold Standard

The Objective Structured Clinical Examination has been the standard in medical education since 1975, used by every accredited medical school worldwide.[^44] Students rotate through 5–20 stations, each testing a specific clinical skill with standardized patients and predetermined marking criteria. OSCEs solved the reliability problem that plagues unstructured oral exams: in the 1960s, correlation between independent clinical evaluators was less than 0.25. With standardized rubrics, OSCEs achieve reliability comparable to multiple-choice tests while measuring competencies writing cannot touch: bedside manner, diagnostic reasoning under pressure, and interpersonal communication.

#### Law: 200 Years of Oral Advocacy

Moot court began at Harvard Law School in 1820 and remains central to legal education worldwide. The UK Bar Training Course requires mandatory oral advocacy in groups of five, with a 60% pass mark set by the Bar Standards Board — proof that oral assessment can have clear, defensible grading standards enforced at the level of professional licensure. The Socratic method, despite its documented decline since its Harvard peak, remains more prominent in legal education than in any undergraduate discipline.

#### Business: The Case Interview as Hiring Filter

McKinsey, BCG, and Bain use 30–60 minute oral problem-solving conversations as their primary hiring mechanism. Candidates must ask clarifying questions, structure problems, perform quantitative analysis verbally, and defend recommendations under questioning. These firms interview thousands of candidates globally using structured rubrics and calibrated interviewers. The case interview tests precisely what written case studies cannot: thinking on your feet, adaptability to curveballs, and the ability to articulate complex reasoning in real time. When corporate recruiters say Harvard graduates are "articulate and decisive," they are describing skills that HBS develops but the College largely does not assess.

#### The Professional-Academic Mismatch

The existence of a $3,000–$6,000 per session executive communication training market proves that universities are failing to develop oral competencies their graduates need. Harvard could capture this value by integrating oral assessment into the curriculum. Instead of graduates paying thousands for media training bootcamps, they would develop oral fluency through four years of scaffolded practice. The irony is stark: corporations spend billions training Harvard graduates in skills Harvard could have taught them.

## Implementation: A Practical Framework

The evidence does not point toward "oral exams everywhere." It points toward appropriate assessment for learning outcomes. Oral formats work best for assessing depth, synthesis, and professional communication — not breadth or factual recall. The following framework offers three tiers of adoption, from minimal disruption to structural transformation.

#### Tier 1: Immediate (Low Lift)

**Add oral verification to existing written work.** Adopt Cornell's model: reserve the right to ask any student to explain their submitted work in a 5–10 minute conversation. The expectation alone deters pure AI generation; the occasional spot-check provides verification. This requires no curriculum change, no scheduling infrastructure, and no rubric development — just a syllabus statement and office hours.

**Use AI practice tools for low-stakes preparation.** Platforms like InStage, VirtualSpeech, and Georgia Tech's Socratic Mind allow students to practice oral argumentation with AI feedback before facing human examiners. Assigning weekly AI-assisted practice sessions normalizes oral performance as routine academic work and reduces anxiety through repeated exposure.

**Record oral assignments for flexible scheduling.** Asynchronous recorded oral submissions (students record themselves explaining a thesis, defending a lab report, or synthesizing readings) can be reviewed by teaching fellows on their own schedules, reducing the logistical burden of synchronous oral exams.

#### Tier 2: Medium-Term (Infrastructure Building)

**Develop shared rubrics.** Drawing on History & Literature's six-category model, Social Studies' comprehensive oral format, and the OSCE's standardized criteria, the Bok Center could facilitate the creation of discipline-specific oral assessment rubrics. The key design principle from SoTL: rubrics should score content understanding and reasoning quality, not accent, grammar, or verbal polish.

**Train teaching fellows.** The University of Melbourne's framework for viva voce assessment provides a tested model: always two examiners present, sessions recorded, base questions predetermined with room for follow-up probing. The Bok Center could offer workshops each semester training TFs in oral assessment techniques, bias awareness, and rubric calibration.

**Deploy scheduling tools for large courses.** Standard scheduling software can manage the logistics of 15–20 minute individual oral sessions across courses with 50–200 students. For courses above 200, the model shifts to section-level oral assessment conducted by trained TFs, with faculty moderating a sample for calibration.

#### Tier 3: Longer-Term (Institutional Transformation)

**Create a working group modeled on Cornell's PWGIA** (Provost's Working Group on Innovation in Assessment) to coordinate assessment redesign across FAS. Cornell's group explicitly focuses on "alternative and authentic forms of assessment" with faculty fellows — a structure that distributes the intellectual labor of redesign rather than placing it on individual instructors.

**Transfer professional school methods to FAS.** HBS's case method rubrics, HMS's OSCE station design, and HLS's Socratic questioning techniques all exist within the University. A cross-school knowledge transfer initiative could adapt these proven models for undergraduate education, respecting disciplinary differences while leveraging shared expertise.

**Designate "Speaking Intensive" courses.** The Communication Across the Curriculum movement provides a tested institutional model: designate certain courses as "SI" (Speaking Intensive) where oral assessment comprises at least 50% of evaluation, parallel to how Writing Intensive courses emphasize written work. This normalizes oral assessment as core pedagogy rather than an AI-era panic response.

**Position Harvard as a leader.** No major American research university has yet produced a comprehensive framework for oral assessment in the AI era. Harvard has the history (founded on oral disputation), the infrastructure (tutorial system, professional school expertise), the resources (Bok Center, favorable student-faculty ratio), and the institutional reputation to define best practice for the sector. The *Washington Post* has already documented a national "renaissance" of oral assessment[^45] — the question is whether Harvard will lead the movement or follow it.

## The Reframe: From Crisis Response to Educational Recovery

The strongest version of the argument for oral assessment has nothing to do with artificial intelligence.

Oral assessment cultivates genuine understanding through the production effect and the protégé effect. It tests cognitive capacities that writing cannot reach: real-time synthesis, responsive argumentation, and the ability to think with an interlocutor. It aligns academic assessment with professional reality in law, medicine, business, policy, journalism, architecture, and every other field where competency is performed, not just documented. It recovers a 2,400-year philosophical tradition from Socrates through Cicero to Augustine in which dialogue — not monologue — was understood as the medium through which genuine knowledge is tested and transmitted.

The AI disruption is real, and the AI-resistance of oral assessment is a genuine practical benefit. But the deeper case is that we abandoned a superior assessment practice for logistical reasons that no longer apply, and a technology-induced crisis has given us the institutional permission to recover it.

Harvard was founded on oral disputation. Its professional schools never abandoned it. Its most rigorous undergraduate concentrations have practiced it for decades. The question is not whether oral assessment belongs in the curriculum but why, for a brief historical moment, we imagined it did not.

---

*This document was prepared by the Bok Center for Teaching and Learning in collaboration with the Learning Lab, synthesizing research from an automated multi-cycle literature review covering 65+ sources across eight thematic domains. February 2026.*

[^1]: HEPI, 'Students and AI: 2025 Survey,' Higher Education Policy Institute (2025).
[^2]: Hua Hsu, 'What Happens After A.I. Destroys College Writing?' The New Yorker (June 2025).
[^3]: Christopher Stray, 'A Peculiar Institution: The Examination in 19th-Century Cambridge,' History of Universities (2001).
[^4]: Harvard Business School, 'The HBS Case Method.'
[^5]: Harden et al., 'Assessment of Clinical Competence Using an Objective Structured Clinical Examination,' British Medical Journal 1 (1975), 447–451.
[^6]: History & Literature concentration, Harvard University. Program founded 1906.
[^7]: Social Studies concentration, Harvard University. Required for all seniors since 1960.
[^8]: Folklore & Mythology, Harvard University. Program founded 1967.
[^9]: Christopher Stray, 'A Peculiar Institution,' History of Universities (2001).
[^10]: Ibid.
[^11]: Classes Préparatoires aux Grandes Écoles (CPGE). 200+ years continuous tradition.
[^12]: Union Public Service Commission (India): 275-mark viva voce as mandatory final stage.
[^13]: Colin M. MacLeod et al., 'The Production Effect,' Journal of Experimental Psychology: Learning, Memory, and Cognition 36:3 (2010), 671–685.
[^14]: Logan Fiorella & Richard E. Mayer, 'The Relative Benefits of Learning by Teaching and Teaching Expectancy,' Contemporary Educational Psychology 38 (2013), 281–288.
[^15]: Paola Iannone & Adrian Simpson, 'How Do We Assess Our Students?' International Journal of Mathematical Education in Science and Technology 51:8 (2020).
[^16]: Frontiers in Education (2024): Oral exams as 'cognitive apprenticeship.'
[^17]: Review of Educational Research (Sage, January 2025): Meta-analysis on oral-written complementarity.
[^18]: Cambridge MPhil instructor (Reddit, April 2025).
[^19]: McGill Daily, 'Is AI Killing Academic Integrity?' (January 2026).
[^20]: Stanford AIWG, 'Generative AI and Education' (March 2024).
[^21]: National University of Singapore, 'Policy for Use of AI in Teaching and Learning' (August 2024).
[^22]: University of British Columbia Faculty of Arts, 'We're Only Human' program (February 2026).
[^23]: Cornell English Language Support Office, 'AI and Academic Integrity' (September 2025).
[^24]: Cogent Education (Taylor & Francis, December 2023).
[^25]: EdSurge (February 2025): California community college oral assessment findings.
[^26]: History & Literature concentration, Harvard University.
[^27]: Social Studies concentration, Harvard University.
[^28]: Folklore & Mythology, Harvard University.
[^29]: Harvard Business School, 'The HBS Case Method.'
[^30]: Harden et al., British Medical Journal 1 (1975), 447–451.
[^31]: Plato, Phaedrus (c. 370 BCE), trans. B. Jowett.
[^32]: Walter J. Ong, Orality and Literacy (Methuen, 1982).
[^33]: Brian Street, Literacy in Theory and Practice (Cambridge University Press, 1984).
[^34]: Cicero, De Oratore (55 BCE).
[^35]: CQUniversity, Australia (2023).
[^36]: Georgia Tech, 'Socratic Mind' platform.
[^37]: Panos Ipeirotis, NYU Stern School of Business.
[^38]: Classes Préparatoires aux Grandes Écoles (CPGE).
[^39]: Sherpa tool (Stanford undergrads).
[^40]: CU Boulder NSF research; Ohio State AI grading analysis.
[^41]: arXiv (May 2024): ASR bias against disfluent speech.
[^42]: Frontiers in Education (2024): Rubrics eliminate bias in non-anonymous grading.
[^43]: EdSurge (February 2025).
[^44]: Harden et al., British Medical Journal 1 (1975), 447–451.
[^45]: Washington Post (December 2025): National 'renaissance' of oral assessment.
